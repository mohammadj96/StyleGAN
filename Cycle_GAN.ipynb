{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K69ROZTz-EtJ"
      },
      "source": [
        "## implementing a Cycle GAN model to transform apple emojis to those of windows and vice versa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPDbupKLbbzV",
        "outputId": "8435f9ec-75b4-4b35-a382-43c769b53849"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet torchmetrics pytorch_lightning opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6pa0ElBhhuP"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader,random_split\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "from torch.nn import functional as F\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbNYfxMnhjON"
      },
      "outputs": [],
      "source": [
        "# this cell will get and prepare the pictures for us\n",
        "# it is assumed that all imges have the same size(32*32)\n",
        "from torchvision import transforms\n",
        "import os\n",
        "transform_t = transforms.Compose([transforms.Normalize([0,0,0],[255,255,255])])\n",
        "class dataset(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "    def __init__(self, path1,path2, transform=transform_t):\n",
        "\n",
        "        self.applimages,self.windimages = self.get_dataset(path1,path2)\n",
        "    def __len__(self):\n",
        "        return len(self.applimages)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample=self.applimages[idx],self.windimages[idx]\n",
        "        return sample\n",
        "    def get_dataset(self,path1,path2):\n",
        "      #get apple emojis and normalize them\n",
        "        filenames = [name for name in os.listdir(path1)]\n",
        "        images=[]\n",
        "        for i, filename in enumerate(filenames[0:]):\n",
        "            pth=os.path.join(path1, filename)\n",
        "            img = torchvision.io.read_image(pth).type(torch.float)\n",
        "            if img.shape[0]==1:\n",
        "               img=torch.cat((img,img,img))\n",
        "            if img.shape[0]==3:\n",
        "               img_normalized = transform_t(img)\n",
        "               images.append(img_normalized)\n",
        "        appl_images = torch.stack(images).type(torch.float)\n",
        "      #get windows emojis and normalize them\n",
        "        filenames = [name for name in os.listdir(path2)]\n",
        "        images=[]\n",
        "        for i, filename in enumerate(filenames[0:]):\n",
        "            pth=os.path.join(path2, filename)\n",
        "            img = torchvision.io.read_image(pth).type(torch.float)\n",
        "            if img.shape[0]==1:\n",
        "               img=torch.cat((img,img,img))\n",
        "            if img.shape[0]==3:\n",
        "               img_normalized = transform_t(img)\n",
        "               images.append(img_normalized)\n",
        "        wind_images = torch.stack(images).type(torch.float)\n",
        "        return appl_images[0:wind_images.shape[0]],wind_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8rOC4X7hn_Z"
      },
      "outputs": [],
      "source": [
        "# read_data\n",
        "# these datasets contain 32*32 images of apple and windows emojis\n",
        "from torch.utils.data import Dataset, DataLoader,random_split\n",
        "train_data = dataset('/content/drive/MyDrive/Appl__resized','/content/drive/MyDrive/Wind__resized')\n",
        "test_data = dataset('/content/drive/MyDrive/Test_Appl_resized','/content/drive/MyDrive/Test_Wind_resized')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwrJ2r-Vk3Gl"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(dataset=train_data,\n",
        "                                           batch_size=128,\n",
        "                                           shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_data,\n",
        "                                           batch_size=8,\n",
        "                                           shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "aVITD1m0mE8P",
        "outputId": "b534d638-5bf0-4b15-d513-e8fddade8b1c"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "#let's look at one batch of the data\n",
        "def imshow(img):\n",
        "    img = img\n",
        "    npimg = img.numpy()\n",
        "    figure = plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# get some random training images\n",
        "appl,wind = next(iter(test_loader))\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(appl))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "685JedUsmKDs",
        "outputId": "3ddf72b9-0857-4ed2-d827-0ae399e06b2c"
      },
      "outputs": [],
      "source": [
        "imshow(torchvision.utils.make_grid(torchvision.transforms.Resize((32,32))(wind)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0JqYdfdzmGh"
      },
      "outputs": [],
      "source": [
        "# the upsample unit will increase the size of the image...we will use this block to move from the latent space to\n",
        "# a complete image\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1, dropout=True):\n",
        "        super(Upsample, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=nn.InstanceNorm2d),\n",
        "            nn.InstanceNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.dropout_layer = nn.Dropout2d(0.5)\n",
        "\n",
        "    def forward(self, x, shortcut=None):\n",
        "        x = self.block(x)\n",
        "        if self.dropout:\n",
        "            x = self.dropout_layer(x)\n",
        "# when we combine upsample and downsample unit, we want connections from the encoded layers to the\n",
        "# reconstruction layers\n",
        "        if shortcut is not None:\n",
        "            x = torch.cat([x, shortcut], dim=1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfxvqD-Bzm9N",
        "outputId": "f6de97c0-c631-463e-c0a0-1db53ab75386"
      },
      "outputs": [],
      "source": [
        "# example use of upsample unit\n",
        "us = Upsample(3,8)\n",
        "us(wind).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Spou5AZ30dr9"
      },
      "outputs": [],
      "source": [
        "# the dowsample unit's job is to get an image and provide a vector in the latent space that has\n",
        "# the important information of the image\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1, apply_instancenorm=True):\n",
        "        super(Downsample, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=nn.InstanceNorm2d)\n",
        "        self.norm = nn.InstanceNorm2d(out_channels)\n",
        "        self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
        "        self.apply_norm = apply_instancenorm\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.apply_norm:\n",
        "            x = self.norm(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNuhP3_q0kk-",
        "outputId": "09305860-f94b-40f2-84eb-b317e97bc3ce"
      },
      "outputs": [],
      "source": [
        "# example use of downsample unit\n",
        "ds = Downsample(3,8)\n",
        "ds(wind).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEQM9ef81FjO"
      },
      "outputs": [],
      "source": [
        "#the generator of the cycle gan\n",
        "# it uses downsample units and upsample units stacked together\n",
        "class CycleGAN_Unet_Generator(nn.Module):\n",
        "    def __init__(self, filter=1):\n",
        "        super(CycleGAN_Unet_Generator, self).__init__()\n",
        "        self.downsamples = nn.ModuleList([\n",
        "            Downsample(3, filter * 8),\n",
        "            Downsample(filter * 8, filter * 8),\n",
        "            Downsample(filter * 8, filter * 8),\n",
        "            Downsample(filter * 8, filter * 8)\n",
        "        ])\n",
        "\n",
        "        self.upsamples = nn.ModuleList([\n",
        "            Upsample(filter * 8, filter * 8),\n",
        "            Upsample(filter * 16, filter * 8),\n",
        "            Upsample(filter * 16, filter * 8)\n",
        "        ])\n",
        "\n",
        "        self.last = nn.Sequential(\n",
        "            nn.ConvTranspose2d(filter * 16, 3, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        skips = []\n",
        "        for l in self.downsamples:\n",
        "            x = l(x)\n",
        "            skips.append(x)\n",
        "# providing middle connections in the network..we want last layers of the encoder to be\n",
        "# connected to the first layers of the decoder\n",
        "        skips = reversed(skips[:-1])\n",
        "        for l, s in zip(self.upsamples, skips):\n",
        "            x = l(x, s)\n",
        "\n",
        "        out = self.last(x)\n",
        "\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tv5fPEl61JFV",
        "outputId": "d26ca702-dbd4-4e7f-dd18-42763a7d5b57"
      },
      "outputs": [],
      "source": [
        "gen = CycleGAN_Unet_Generator()\n",
        "gen(wind).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaLMRkn4mNX3"
      },
      "outputs": [],
      "source": [
        "# our discriminator\n",
        "# its job should be downsampling an image and checking if it is real or fake\n",
        "class CycleGAN_Discriminator(nn.Module):\n",
        "    def __init__(self, filter=1):\n",
        "        super(CycleGAN_Discriminator, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            Downsample(3, filter*8, kernel_size=4, stride=1, apply_instancenorm=False),\n",
        "            Downsample(filter*8, filter * 8, kernel_size=4, stride=1),\n",
        "            Downsample(filter * 8, filter * 8, kernel_size=4, stride=1),\n",
        "            Downsample(filter * 8, filter * 8, kernel_size=4, stride=1),\n",
        "        )\n",
        "\n",
        "        self.last = nn.Conv2d(filter * 8, 1, kernel_size=3, stride=1, padding=3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block(x)\n",
        "        x = self.last(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2p9rhFiRtkYl",
        "outputId": "9a3978b9-1aea-4971-b126-848ed437dae0"
      },
      "outputs": [],
      "source": [
        "dis = CycleGAN_Discriminator()\n",
        "dis(wind).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xf-uly3jmz-B"
      },
      "outputs": [],
      "source": [
        "# CycleGAN - Lightning Module ---------------------------------------------------------------------------\n",
        "class CycleGAN_LightningSystem(LightningModule):\n",
        "    def __init__(self, G_basestyle, G_stylebase, D_base, D_style, lr, reconstr_w=10, id_w=2,samples=test_data.applimages[0:4]):\n",
        "        super(CycleGAN_LightningSystem, self).__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.automatic_optimization = False\n",
        "        self.samples=samples #we will use them to track the performance of the model\n",
        "        # define our generators\n",
        "        self.G_basestyle = G_basestyle\n",
        "        self.G_stylebase = G_stylebase\n",
        "        # define our discriminators\n",
        "        self.D_base = D_base\n",
        "        self.D_style = D_style\n",
        "        self.lr = 0.0002\n",
        "        self.reconstr_w = reconstr_w\n",
        "        self.id_w = id_w\n",
        "        self.cnt_train_step = 0\n",
        "        self.step = 0\n",
        "\n",
        "        self.mae = nn.L1Loss()\n",
        "        self.generator_loss = nn.MSELoss()\n",
        "        self.discriminator_loss = nn.MSELoss()\n",
        "        self.losses = []\n",
        "        self.G_mean_losses = []\n",
        "        self.D_mean_losses = []\n",
        "        self.validity = []\n",
        "        self.reconstr = []\n",
        "        self.identity = []\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "# we will train both generators with one optimizer and both discriminators with another one\n",
        "        parameters1 = list(self.G_basestyle.parameters()) + list(self.G_stylebase.parameters())\n",
        "        g_optimizer = torch.optim.Adam(parameters1, lr=self.lr)\n",
        "        parameters2 = list(self.D_base.parameters()) + list(self.D_style.parameters())\n",
        "        d_optimizer = torch.optim.Adam(parameters2, lr=self.lr)\n",
        "        return [g_optimizer, d_optimizer], []\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        base_img, style_img = batch\n",
        "        g_optimizer, d_optimizer = self.optimizers()\n",
        "        self.toggle_optimizer(g_optimizer)\n",
        "        b = base_img.shape[0]\n",
        "        valid = torch.ones(b, 1, 32, 32)\n",
        "        fake = torch.zeros(b, 1, 32, 32)\n",
        "\n",
        "        # Train Generators:\n",
        "            # Validity\n",
        "            # MSELoss\n",
        "        val_base = self.generator_loss(self.D_base(self.G_stylebase(style_img)), valid)\n",
        "        val_style = self.generator_loss(self.D_style(self.G_basestyle(base_img)), valid)\n",
        "        val_loss = (val_base + val_style) / 2\n",
        "\n",
        "        # Reconstruction\n",
        "        reconstr_base = self.mae(self.G_stylebase(self.G_basestyle(base_img)), base_img)\n",
        "        reconstr_style = self.mae(self.G_basestyle(self.G_stylebase(style_img)), style_img)\n",
        "        reconstr_loss = (reconstr_base + reconstr_style) / 2\n",
        "\n",
        "        # Identity\n",
        "        id_base = self.mae(self.G_stylebase(base_img), base_img)\n",
        "        id_style = self.mae(self.G_basestyle(style_img), style_img)\n",
        "        id_loss = (id_base + id_style) / 2\n",
        "\n",
        "        # Loss Weight\n",
        "        G_loss = val_loss + self.reconstr_w * reconstr_loss + self.id_w * id_loss\n",
        "\n",
        "        self.log(\"g_loss\", G_loss, prog_bar=True)\n",
        "        self.manual_backward(G_loss)\n",
        "        g_optimizer.step()\n",
        "        g_optimizer.zero_grad()\n",
        "        self.untoggle_optimizer(g_optimizer)\n",
        "\n",
        "\n",
        "\n",
        "        self.toggle_optimizer(d_optimizer)\n",
        "        # Train Discriminators:\n",
        "            # MSELoss\n",
        "        D_base_gen_loss = self.discriminator_loss(self.D_base(self.G_stylebase(style_img).detach()), fake)\n",
        "        D_style_gen_loss = self.discriminator_loss(self.D_style(self.G_basestyle(base_img).detach()), fake)\n",
        "        D_base_valid_loss = self.discriminator_loss(self.D_base(base_img), valid)\n",
        "        D_style_valid_loss = self.discriminator_loss(self.D_style(style_img), valid)\n",
        "\n",
        "\n",
        "            # Loss Weight\n",
        "        D_loss = ((D_base_gen_loss + D_style_gen_loss) + D_base_valid_loss + D_style_valid_loss) / 4\n",
        "\n",
        "        self.log(\"d_loss\", D_loss, prog_bar=True)\n",
        "        self.manual_backward(D_loss)\n",
        "        d_optimizer.step()\n",
        "        d_optimizer.zero_grad()\n",
        "        self.untoggle_optimizer(d_optimizer)\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "            # Display Model Output\n",
        "            target_imgs = self.samples.detach()\n",
        "            gen_imgs = self.G_basestyle(target_imgs)\n",
        "            gen_img = torch.cat([target_imgs, gen_imgs], dim=0)\n",
        "\n",
        "            gen_img = gen_img * 255\n",
        "\n",
        "            joined_images_tensor = torchvision.utils.make_grid(gen_img, nrow=4, padding=2)\n",
        "\n",
        "            joined_images = joined_images_tensor.detach().cpu().numpy().astype(int)\n",
        "            joined_images = np.transpose(joined_images, [1,2,0])\n",
        "\n",
        "            # Visualize\n",
        "            fig = plt.figure(figsize=(4, 4))\n",
        "            plt.imshow(joined_images)\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e3c27c60a6a24adfb47fada45c2e6c35",
            "41b362c1e52b4f608d8a0f66fbedba03",
            "dac2d854f84245e1b46e4a1fca37f59f",
            "768d87b27cb045c2aef7e6bd39754ede",
            "2109b3ea43e14814af347e8cd97fd5ee",
            "01c184b21af3463f92ac3c63437257ee",
            "5756b4f5b4b744609f21dfc0714c0c50",
            "d4d1f7bf56ed41079fdd84a02a577f32",
            "d37febffbcc742d993e98f594c328800",
            "dd4eca45496a40d18fae8c633b5c9e6c",
            "d525373d95c2422a8a643283e55a218c"
          ]
        },
        "id": "RjEmxiN9nBGV",
        "outputId": "c6b70a80-39d5-4a55-f3d0-b603e3e85c6a"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "lr = {\n",
        "    'G': 0.0002,\n",
        "    'D': 0.0002\n",
        "}\n",
        "epoch = 10\n",
        "seed = 42\n",
        "reconstr_w = 10\n",
        "id_w = 2\n",
        "\n",
        "\n",
        "# DataModule  -----------------------------------------------------------------\n",
        "train_loader = DataLoader(dataset=train_data,\n",
        "                                           batch_size=128,\n",
        "                                           shuffle=True)\n",
        "G_basestyle = CycleGAN_Unet_Generator()\n",
        "G_stylebase = CycleGAN_Unet_Generator()\n",
        "D_base = CycleGAN_Discriminator()\n",
        "D_style = CycleGAN_Discriminator()\n",
        "\n",
        "\n",
        "# LightningModule  --------------------------------------------------------------\n",
        "model = CycleGAN_LightningSystem(G_basestyle, G_stylebase, D_base, D_style,\n",
        "                                 lr, reconstr_w, id_w)\n",
        "\n",
        "# Trainer  --------------------------------------------------------------\n",
        "trainer = Trainer(\n",
        "    max_epochs=10,\n",
        "    num_sanity_val_steps=0)\n",
        "\n",
        "\n",
        "# Train\n",
        "trainer.fit(model, train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KPue5kpC8iv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01c184b21af3463f92ac3c63437257ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2109b3ea43e14814af347e8cd97fd5ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "41b362c1e52b4f608d8a0f66fbedba03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01c184b21af3463f92ac3c63437257ee",
            "placeholder": "​",
            "style": "IPY_MODEL_5756b4f5b4b744609f21dfc0714c0c50",
            "value": "Epoch 9: 100%"
          }
        },
        "5756b4f5b4b744609f21dfc0714c0c50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "768d87b27cb045c2aef7e6bd39754ede": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd4eca45496a40d18fae8c633b5c9e6c",
            "placeholder": "​",
            "style": "IPY_MODEL_d525373d95c2422a8a643283e55a218c",
            "value": " 16/16 [00:25&lt;00:00,  0.63it/s, v_num=0, g_loss=3.310, d_loss=0.276]"
          }
        },
        "d37febffbcc742d993e98f594c328800": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4d1f7bf56ed41079fdd84a02a577f32": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d525373d95c2422a8a643283e55a218c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dac2d854f84245e1b46e4a1fca37f59f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4d1f7bf56ed41079fdd84a02a577f32",
            "max": 16,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d37febffbcc742d993e98f594c328800",
            "value": 16
          }
        },
        "dd4eca45496a40d18fae8c633b5c9e6c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3c27c60a6a24adfb47fada45c2e6c35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_41b362c1e52b4f608d8a0f66fbedba03",
              "IPY_MODEL_dac2d854f84245e1b46e4a1fca37f59f",
              "IPY_MODEL_768d87b27cb045c2aef7e6bd39754ede"
            ],
            "layout": "IPY_MODEL_2109b3ea43e14814af347e8cd97fd5ee"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
